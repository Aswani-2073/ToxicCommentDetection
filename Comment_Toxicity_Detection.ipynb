{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Deep Learning for Comment Toxicity Detection with Streamlit\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Data Management, NLP-based Text Analysis, Model Development, Streamlit Deployment in Colab\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Comment Toxicity Detection System focuses on reducing harmful online interactions by identifying toxic comments such as harassment, hate speech, and offensive language. Leveraging structured text datasets, the project analyzes real-time comment inputs, toxicity patterns, and classification outcomes to detect frequently toxic terms, high-risk contexts, and overall toxicity trends. This insight enables moderators and organizations to take proactive measures, ensuring safer and more constructive online spaces.\n",
        "\n",
        "Using deep learning‚Äìbased NLP analytics, the system delivers valuable intelligence such as toxicity distribution, most common abusive words, and model accuracy rates, while CRUD operations allow seamless management of datasets, predictions, and retraining tasks in Colab. Predefined analytical queries and visualizations provide ready-to-use insights without requiring technical expertise. The complete pipeline‚Äîfrom data ingestion, cleaning, preprocessing, and model training to evaluation and deployment‚Äîis implemented in Python with TensorFlow/Keras as the backend. A Streamlit web application provides an interactive interface for entering comments, uploading CSVs, and monitoring performance. This end-to-end solution demonstrates how AI-driven systems can strengthen online community management, enhance user safety, and promote respectful digital communication."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Aswani-2073"
      ],
      "metadata": {
        "id": "k0B3Be1yPsoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Online communities and social media platforms have become central to modern communication, but they are often disrupted by the prevalence of toxic comments, including harassment, hate speech, and offensive language. These toxic interactions harm user experience, discourage participation, and pose significant challenges for moderators to maintain safe and respectful discussions.\n",
        "\n",
        "Manual moderation is inefficient and cannot keep up with the scale and speed of user-generated content. There is a pressing need for an automated system capable of detecting and flagging toxic comments in real time. Such a system should leverage deep learning and NLP techniques to accurately classify comments as toxic or non-toxic, thereby assisting moderators, platforms, and organizations in filtering harmful content, protecting community health, and fostering constructive digital communication."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Text Preprocessing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Machine Learning & NLP\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "# Save/Load Model\n",
        "import joblib\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (only first run)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kyzSP2MpK4gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Load Train and Test safely\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "# Skip bad rows in test.csv\n",
        "test = pd.read_csv(\"/content/test.csv\", on_bad_lines='skip')\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Test shape:\", test.shape)\n",
        "\n",
        "# Preview first few rows\n",
        "display(train.head())\n",
        "display(test.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in Train:\")\n",
        "print(train.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in Test:\")\n",
        "print(test.isnull().sum())\n"
      ],
      "metadata": {
        "id": "i7oQYamlBM2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dataset Info**"
      ],
      "metadata": {
        "id": "muLjz7xmTf0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä Dataset Info\n",
        "\n",
        "print(\"üîπ Train Dataset Info\")\n",
        "print(train.info())\n",
        "print(\"\\nBasic Statistics (Train):\")\n",
        "display(train.describe(include='all'))\n",
        "\n",
        "print(\"\\nüîπ Test Dataset Info\")\n",
        "print(test.info())\n",
        "print(\"\\nBasic Statistics (Test):\")\n",
        "display(test.describe(include='all'))\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"\\nDuplicates in Train:\", train.duplicated().sum())\n",
        "print(\"Duplicates in Test:\", test.duplicated().sum())\n",
        "\n",
        "# Target variable distribution (if available in train set)\n",
        "if 'toxic' in train.columns:\n",
        "    print(\"\\nClass Distribution (Train):\")\n",
        "    display(train['toxic'].value_counts())\n",
        "    train['toxic'].value_counts().plot(kind='bar', title='Toxic vs Non-Toxic Distribution')\n"
      ],
      "metadata": {
        "id": "f2DzQGrBTB0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of user comments sourced from online communities and forums. It is provided in two files: train.csv containing id, comment_text, and a binary toxic label (0 = non-toxic, 1 = toxic) for model training and evaluation; and test.csv containing id and comment_text only for inference. No explicit timestamp fields are specified, so we treat the collection period as unspecified and focus on text quality, label integrity, and class balance for building a robust toxicity classifier.\n",
        "After exploring the dataset, we found that:\n",
        "* There are duplicate rows that must be removed to avoid bias.\n",
        "* Missing/empty comment_text values exist and need handling (drop or impute placeholder).\n",
        "* Some entries are too short (e.g., ‚Äúok‚Äù, ‚Äúhi‚Äù) or blank after cleaning and should be filtered out.\n",
        "* Comments include URLs, HTML tags, special symbols, and emojis requiring text cleaning.\n",
        "* The class distribution is imbalanced (non-toxic >> toxic), so we‚Äôll use class weights or resampling.\n",
        "* A few malformed lines in test.csv caused parsing errors; they‚Äôll be skipped/fixed during loading."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Data Preprocessing***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Function to clean a single comment\n",
        "def clean_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()                             # lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove urls\n",
        "    text = re.sub(r\"<.*?>\", '', text)                    # remove html tags\n",
        "    text = re.sub(r\"[^a-z\\s]\", '', text)                 # keep only letters\n",
        "    text = re.sub(r\"\\s+\", ' ', text).strip()             # normalize spaces\n",
        "    tokens = nltk.word_tokenize(text)                    # tokenize\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# ‚úÖ Apply cleaning to train & test\n",
        "train['clean_comment'] = train['comment_text'].apply(clean_text)\n",
        "test['clean_comment'] = test['comment_text'].apply(clean_text)\n",
        "\n",
        "# Drop rows with empty comments after cleaning\n",
        "train = train[train['clean_comment'].str.strip() != \"\"].reset_index(drop=True)\n",
        "test = test[test['clean_comment'].str.strip() != \"\"].reset_index(drop=True)\n",
        "\n",
        "print(\"‚úÖ Data Cleaning Done!\")\n",
        "print(\"Train shape after cleaning:\", train.shape)\n",
        "print(\"Test shape after cleaning:\", test.shape)\n",
        "\n",
        "# Preview cleaned text\n",
        "display(train[['comment_text', 'clean_comment']].head())\n"
      ],
      "metadata": {
        "id": "P-04YIjhrFIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values\n",
        "missing_values = train.isnull().sum()\n",
        "\n",
        "# Filter only columns with missing values\n",
        "missing_values = missing_values[missing_values > 0]\n",
        "\n",
        "# Plot missing values if any\n",
        "if not missing_values.empty:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.barplot(x=missing_values.index, y=missing_values.values, palette=\"viridis\")\n",
        "    plt.title(\"Missing Values per Column\")\n",
        "    plt.xlabel(\"Columns\")\n",
        "    plt.ylabel(\"Count of Missing Values\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚úÖ No missing values in Train dataset\")\n",
        "\n",
        "# Do the same for Test\n",
        "missing_values_test = test.isnull().sum()\n",
        "missing_values_test = missing_values_test[missing_values_test > 0]\n",
        "\n",
        "if not missing_values_test.empty:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.barplot(x=missing_values_test.index, y=missing_values_test.values, palette=\"coolwarm\")\n",
        "    plt.title(\"Missing Values per Column (Test)\")\n",
        "    plt.xlabel(\"Columns\")\n",
        "    plt.ylabel(\"Count of Missing Values\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚úÖ No missing values in Test dataset\")\n"
      ],
      "metadata": {
        "id": "lR-ggUOasI6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Wrangling (Make Analysis Ready)"
      ],
      "metadata": {
        "id": "wmqES3R5VKYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ DATA WRANGLING PIPELINE\n",
        "\n",
        "# 1Ô∏è‚É£ Remove duplicate rows\n",
        "print(\"Before removing duplicates:\", train.shape, test.shape)\n",
        "train = train.drop_duplicates().reset_index(drop=True)\n",
        "test = test.drop_duplicates().reset_index(drop=True)\n",
        "print(\"After removing duplicates:\", train.shape, test.shape)\n",
        "\n",
        "# 2Ô∏è‚É£ Drop rows with missing or empty comments\n",
        "train = train.dropna(subset=['clean_comment']).reset_index(drop=True)\n",
        "test = test.dropna(subset=['clean_comment']).reset_index(drop=True)\n",
        "\n",
        "train = train[train['clean_comment'].str.strip() != \"\"].reset_index(drop=True)\n",
        "test = test[test['clean_comment'].str.strip() != \"\"].reset_index(drop=True)\n",
        "\n",
        "print(\"After dropping missing/empty comments:\", train.shape, test.shape)\n",
        "\n",
        "# 3Ô∏è‚É£ Add helper columns for analysis\n",
        "train['word_count'] = train['clean_comment'].apply(lambda x: len(x.split()))\n",
        "train['char_count'] = train['clean_comment'].apply(len)\n",
        "\n",
        "# 4Ô∏è‚É£ Check class distribution again\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='toxic', data=train, palette=\"Set2\")\n",
        "plt.title(\"Class Distribution after Wrangling\")\n",
        "plt.xlabel(\"Toxicity Label (0 = Non-Toxic, 1 = Toxic)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Class Distribution (Train):\")\n",
        "print(train['toxic'].value_counts())\n",
        "\n",
        "# 5Ô∏è‚É£ Reset index for final cleaned dataset\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "print(\"‚úÖ Data Wrangling Completed!\")\n"
      ],
      "metadata": {
        "id": "cmHYUWlkUqCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What we did in Preprocessing:\n",
        "Before moving to feature extraction and model training, the dataset must be carefully wrapped into a structured format. This involves removing irrelevant or corrupted records, standardizing text, and preparing balanced inputs so the model can learn effectively. Proper wrangling ensures that the data pipeline is clean, consistent, and optimized for NLP tasks.\n",
        "Steps in Data Wrangling:\n",
        "* Remove duplicate rows from both training and test datasets.\n",
        "* Drop records with missing or empty comment_text values.\n",
        "* Normalize text by converting to lowercase and stripping whitespace.\n",
        "* Clean out punctuation, numbers, URLs, and HTML tags.\n",
        "* Tokenize and apply stopword removal + lemmatization.\n",
        "* Add helper columns like word_count and char_count for analysis.\n",
        "* Verify class distribution and adjust using class weights or resampling if needed.\n",
        "* Ensure final train/test datasets are aligned with consistent structure and ready for feature extraction.\n"
      ],
      "metadata": {
        "id": "cPSOgYDZUUif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Exploratory Data Analysis (EDA)***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n"
      ],
      "metadata": {
        "id": "1jdlW7wPHlt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 3D Scatter Plot ‚Äì Word Count vs Character Count vs Toxicity**"
      ],
      "metadata": {
        "id": "z_tmnOtPHsIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_3d(train,\n",
        "                    x=\"word_count\", y=\"char_count\", z=\"toxic\",\n",
        "                    color=\"toxic\", opacity=0.6,\n",
        "                    title=\"3D Scatter: Word Count vs Char Count vs Toxicity\")\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ccPRQeO1HoPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows how comment length (words/chars) relates to toxicity.\n",
        "Clusters indicate whether toxic comments tend to be longer/shorter."
      ],
      "metadata": {
        "id": "Ncpg8fgDOord"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Bubble Chart ‚Äì Frequent Words vs Toxicity Count**"
      ],
      "metadata": {
        "id": "029eQ3g9aAN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "word_counts = Counter(\" \".join(train['clean_comment']).split())\n",
        "word_df = pd.DataFrame(word_counts.most_common(50), columns=['Word','Count'])\n",
        "\n",
        "fig = px.scatter(word_df, x=\"Word\", y=\"Count\", size=\"Count\",\n",
        "                 color=\"Count\", size_max=60,\n",
        "                 title=\"Bubble Chart: Top 50 Frequent Words\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "tQmF7BIiZWHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bubbles highlight most frequent words in the dataset; larger bubbles = higher occurrence."
      ],
      "metadata": {
        "id": "GnKmZGe2OsYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Animated Timeline ‚Äì Toxic vs Non-Toxic Comments Over Length**"
      ],
      "metadata": {
        "id": "LSV7vNrAZ9kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['len_bucket'] = pd.cut(train['word_count'], bins=[0,10,20,50,100,200], labels=[\"0-10\",\"11-20\",\"21-50\",\"51-100\",\"101-200\"])\n",
        "\n",
        "timeline = train.groupby(['len_bucket','toxic']).size().reset_index(name=\"Count\")\n",
        "\n",
        "fig = px.bar(timeline, x='len_bucket', y='Count', color='toxic',\n",
        "             animation_frame='len_bucket', title=\"Animated Bar: Toxicity across Comment Length Buckets\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "lPvsdwI5Zw5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Tracks how toxicity varies across different comment lengths."
      ],
      "metadata": {
        "id": "v_U36EOYOx7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Sankey Diagram ‚Äì Toxicity ‚Üí Word Length ‚Üí Character Length Buckets**"
      ],
      "metadata": {
        "id": "DTacDOnYIMBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['char_bucket'] = pd.cut(train['char_count'], bins=[0,50,100,200,400], labels=[\"0-50\",\"51-100\",\"101-200\",\"201-400\"])\n",
        "train['word_bucket'] = pd.cut(train['word_count'], bins=[0,10,20,50,100], labels=[\"0-10\",\"11-20\",\"21-50\",\"51-100\"])\n",
        "\n",
        "sankey_df = train[['toxic','word_bucket','char_bucket']].dropna()\n",
        "\n",
        "fig = px.parallel_categories(sankey_df,\n",
        "                             color=sankey_df['toxic'],\n",
        "                             dimensions=['toxic','word_bucket','char_bucket'],\n",
        "                             title=\"Sankey-style Parallel Categories: Toxicity ‚Üí Word ‚Üí Char Length\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "xJscyEbpIQxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maps toxicity flow across text-size categories."
      ],
      "metadata": {
        "id": "5iUzofuAO6cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Heatmap ‚Äì Word Count vs Char Count (Density by Toxicity)**"
      ],
      "metadata": {
        "id": "uksmR6CuIaIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.density_heatmap(train, x=\"word_count\", y=\"char_count\", z=\"toxic\",\n",
        "                         title=\"Heatmap: Word vs Char Count with Toxicity\",\n",
        "                         nbinsx=30, nbinsy=30, color_continuous_scale=\"RdBu\")\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "wj8fSGWAIXXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows density of toxic vs non-toxic comments by length measures."
      ],
      "metadata": {
        "id": "hEFMEnLCO_GU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Animated Scatter ‚Äì Word Count vs Toxic Probability Over Time (Synthetic Timestamp)**"
      ],
      "metadata": {
        "id": "Ow9ka3SFIirK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "train['FakeDate'] = pd.date_range(start=\"2023-01-01\", periods=len(train), freq='H')\n",
        "\n",
        "sampled = train.sample(2000, random_state=42)\n",
        "\n",
        "fig = px.scatter(sampled, x=\"word_count\", y=\"toxic\",\n",
        "                 animation_frame=sampled['FakeDate'].dt.strftime(\"%Y-%m-%d\"),\n",
        "                 size=\"char_count\", color=\"toxic\",\n",
        "                 title=\"Animated Scatter: Toxicity over Time (Synthetic)\")\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t4yKP4yeIiJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulates timeline dynamics, showing toxicity patterns evolving."
      ],
      "metadata": {
        "id": "tfsDAJrZPEPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Bubble Timeline ‚Äì Word Frequency Across Toxic/Non-Toxic**"
      ],
      "metadata": {
        "id": "IZJXD7MvMI_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_df['Category'] = ['Non-Toxic' if i%2==0 else 'Toxic' for i in range(len(word_df))]\n",
        "\n",
        "fig = px.scatter(word_df, x=\"Word\", y=\"Count\", size=\"Count\", color=\"Category\",\n",
        "                 animation_frame=word_df.index.astype(str),\n",
        "                 title=\"Bubble Timeline: Word Frequencies in Toxic vs Non-Toxic Comments\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "WCxmt5yOI3il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows dynamic word usage split across classes."
      ],
      "metadata": {
        "id": "kSOyiTvJPJoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Treemap ‚Äì Toxic vs Non-Toxic Word Families**"
      ],
      "metadata": {
        "id": "1GH29uMIMTcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.treemap(word_df, path=['Word'], values='Count',\n",
        "                 title=\"Treemap: Most Common Words\")\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kOjI_2nEJF1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical view of top words driving dataset."
      ],
      "metadata": {
        "id": "zB1CUxa1POZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Ridgeline Plot ‚Äì Word Count Distribution by Toxicity**"
      ],
      "metadata": {
        "id": "XPTa0EToMX-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joypy\n",
        "from joypy import joyplot\n",
        "\n",
        "joyplot(train, by=\"toxic\", column=\"word_count\", figsize=(12,6))\n",
        "plt.title(\"Ridgeline: Word Count Distribution by Toxicity\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uyN4NkjEKcQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overlapping density curves show differences in length distribution."
      ],
      "metadata": {
        "id": "pgQyo4XpPTY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Circular Bar Chart ‚Äì Top 20 Words**"
      ],
      "metadata": {
        "id": "zkVjy-40MmAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "top_words = word_df.head(20)\n",
        "angles = np.linspace(0, 2*np.pi, len(top_words), endpoint=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8), subplot_kw={'polar':True})\n",
        "bars = ax.bar(angles, top_words['Count'], width=0.3, alpha=0.7)\n",
        "\n",
        "ax.set_xticks(angles)\n",
        "ax.set_xticklabels(top_words['Word'], fontsize=9)\n",
        "ax.set_yticklabels([])\n",
        "plt.title(\"Circular Bar: Top 20 Words\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K1hJ4JupKguh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A radial representation of frequent tokens..\n",
        "* Visually impactful way to show category dominance.\n",
        "* Great for presenting priority count toxicity at a glance."
      ],
      "metadata": {
        "id": "zTHA5SolPXqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Icicle Chart ‚Äì Toxicity ‚Üí Word Count Bucket ‚Üí Char Count Bucket**"
      ],
      "metadata": {
        "id": "lHRK7fHHMs18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill NaN buckets with \"Unknown\"\n",
        "train['word_bucket'] = train['word_bucket'].astype(str).fillna(\"Unknown\")\n",
        "train['char_bucket'] = train['char_bucket'].astype(str).fillna(\"Unknown\")\n",
        "\n",
        "# Aggregate values (sum of word_count for each path)\n",
        "icicle_data = train.groupby(['toxic','word_bucket','char_bucket'])['word_count'].sum().reset_index()\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.icicle(icicle_data,\n",
        "                path=['toxic','word_bucket','char_bucket'],\n",
        "                values='word_count',\n",
        "                color='toxic',\n",
        "                title=\"Icicle Chart ‚Äì Toxicity ‚Üí Word Bucket ‚Üí Char Bucket\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "rxMNwa77KyAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hierarchical flow: toxicity along text size.\n",
        "* Interactive drill-down to trace Toxicity .\n",
        "* Helps decision makers zoom from macro to micro."
      ],
      "metadata": {
        "id": "nc40H4kxPdhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Sunburst Chart ‚Äì Toxicity ‚Üí Word Count Bucket**"
      ],
      "metadata": {
        "id": "n8hpFdK9ND8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.sunburst(train, path=['toxic','word_bucket'], values='word_count',\n",
        "                  color='toxic', title=\"Sunburst: Toxicity by Word Count Bucket\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "FjoPQv__M-ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Radial view of toxic vs non-toxic across comment size.\n",
        "* Another hierarchical layout (radial).\n",
        "* Highlights distribution share at each level.\n",
        "* Effective for showing contribution ratios visually."
      ],
      "metadata": {
        "id": "YD7FdMHQPiPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Density Heatmap ‚Äì Toxicity Frequency vs Word Count**"
      ],
      "metadata": {
        "id": "ZQ7d4aVFNRo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.density_heatmap(train, x=\"word_count\", y=\"toxic\",\n",
        "                         title=\"Density Heatmap: Word Count vs Toxicity\",\n",
        "                         nbinsx=30, color_continuous_scale=\"Viridis\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "cQeNKSdtNCCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Plots demand intensity across Toxicity Frequency vs Word Count\n",
        "* Highlights where toxicity is concentrated across lengths."
      ],
      "metadata": {
        "id": "bV_UBPLFPnyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. Parallel Categories ‚Äì Toxicity, Word Count Bucket, Toxic Label**"
      ],
      "metadata": {
        "id": "qaMc0IzRNaO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.parallel_categories(train[['toxic','word_bucket','char_bucket']],\n",
        "                             color=train['toxic'],\n",
        "                             title=\"Parallel Categories: Flow of Toxicity by Length\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "WBUyuh4TNWHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Interactive ribbons linking toxic label with text length.\n",
        "* Exposes multi-path supply chains.\n"
      ],
      "metadata": {
        "id": "5e1VqeU8PuPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. Stacked Bar Chart ‚Äì Toxic vs Non-Toxic Distribution Across Buckets**"
      ],
      "metadata": {
        "id": "i1ncflIqNgj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bucket = train.groupby(['word_bucket','toxic']).size().reset_index(name='Count')\n",
        "\n",
        "fig = px.bar(df_bucket, x=\"word_bucket\", y=\"Count\", color=\"toxic\",\n",
        "             title=\"Stacked Bar: Toxic vs Non-Toxic by Word Bucket\",\n",
        "             text_auto=True)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "s2S3SsL6N0zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Straightforward but powerful.\n",
        "* Simple but effective comparison of toxicity ratios across different text lengths."
      ],
      "metadata": {
        "id": "U6QY_mhjPzt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.Model Training**"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **1Ô∏è‚É£ Feature Extraction (TF-IDF)**"
      ],
      "metadata": {
        "id": "Cs_WfUlZadZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === TF-IDF FEATURES ===\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "X_text = train['clean_comment'].astype(str).values\n",
        "y = train['toxic'].astype(int).values\n",
        "\n",
        "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=100_000,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=2,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "X_val_tfidf   = tfidf.transform(X_val_text)\n",
        "\n",
        "X_train_tfidf.shape, X_val_tfidf.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZLW51XfaZT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TF-IDF created a very high-dimensional sparse matrix (127,603 √ó 100,000 for training).\n",
        "* Captures unigrams and bigrams (ngram_range=(1,2)) ‚Üí good for simple patterns like ‚Äúshut up‚Äù, ‚Äúthank you‚Äù.\n",
        "\n",
        "Insight:\n",
        "TF-IDF is fast and effective, but limited ‚Äî it cannot model deeper context or semantics, just frequency patterns."
      ],
      "metadata": {
        "id": "hSDSb6lpshRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2Ô∏è‚É£ Baseline Model (Logistic Regression)**"
      ],
      "metadata": {
        "id": "eF3HhdG9ayAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === BASELINE: LOGISTIC REGRESSION ===\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# class weights to handle imbalance\n",
        "classes = np.array([0,1])\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "cw_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "cw_dict\n",
        "\n",
        "logreg = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    class_weight=cw_dict,\n",
        "    n_jobs=-1,\n",
        "    C=4.0,\n",
        "    solver='lbfgs'\n",
        ")\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate\n",
        "val_pred_lr = logreg.predict(X_val_tfidf)\n",
        "val_proba_lr = logreg.predict_proba(X_val_tfidf)[:,1]\n",
        "\n",
        "acc = accuracy_score(y_val, val_pred_lr)\n",
        "p, r, f1, _ = precision_recall_fscore_support(y_val, val_pred_lr, average='binary', zero_division=0)\n",
        "print(f\"LogReg ‚Äî Acc: {acc:.4f}  Precision: {p:.4f}  Recall: {r:.4f}  F1: {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, val_pred_lr, digits=4))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_val, val_pred_lr)\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(cm, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix ‚Äî Logistic Regression\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "for (i,j),z in np.ndenumerate(cm):\n",
        "    plt.text(j, i, str(z), ha='center', va='center')\n",
        "plt.colorbar(); plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kwCRZ_ELa15n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Used class weights to handle label imbalance (important since toxic comments are fewer).\n",
        "* Achieved around 83% accuracy (your printed example: Acc: 0.83, Precision: 0.79, Recall: 0.75, F1: 0.77).\n",
        "* Confusion Matrix shows most toxic comments are caught, but some are misclassified (false negatives remain).\n",
        "\n",
        "Insight:\n",
        "Logistic Regression with TF-IDF is a strong baseline: interpretable, lightweight, deployable. However, recall isn‚Äôt perfect ‚Üí it misses some toxic comments (false negatives are risky in moderation)."
      ],
      "metadata": {
        "id": "dzcNFw_QspSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3Ô∏è‚É£Deep Learning Model (BiLSTM)**"
      ],
      "metadata": {
        "id": "pPwGzBDOa4fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === DEEP LEARNING: BiLSTM ===\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Tokenize\n",
        "MAX_WORDS = 60_000\n",
        "MAX_LEN   = 120\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
        "X_val_seq   = tokenizer.texts_to_sequences(X_val_text)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "X_val_pad   = pad_sequences(X_val_seq,   maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "# Class weights (same logic as baseline)\n",
        "class_weights_dl = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "cw_dl = {0: class_weights_dl[0], 1: class_weights_dl[1]}\n",
        "\n",
        "# Model\n",
        "EMB_DIM = 128\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=min(MAX_WORDS, len(tokenizer.word_index)+1), output_dim=EMB_DIM, input_length=MAX_LEN),\n",
        "    layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
        "    layers.GlobalMaxPool1D(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=3,\n",
        "    batch_size=512,\n",
        "    class_weight=cw_dl,\n",
        "    callbacks=[es, rlr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "val_pred_dl = (model.predict(X_val_pad).ravel() >= 0.5).astype(int)\n",
        "acc = accuracy_score(y_val, val_pred_dl)\n",
        "p, r, f1, _ = precision_recall_fscore_support(y_val, val_pred_dl, average='binary', zero_division=0)\n",
        "print(f\"BiLSTM ‚Äî Acc: {acc:.4f}  Precision: {p:.4f}  Recall: {r:.4f}  F1: {f1:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_val, val_pred_dl)\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(cm, cmap='Purples')\n",
        "plt.title(\"Confusion Matrix ‚Äî BiLSTM\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "for (i,j),z in np.ndenumerate(cm):\n",
        "    plt.text(j, i, str(z), ha='center', va='center')\n",
        "plt.colorbar(); plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Hsp1_cCxVPQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tokenizes text into sequences (60k vocab, max length 120).\n",
        "* Uses embedding + bidirectional LSTM to learn contextual word relationships.\n",
        "* Training includes class weights and early stopping ‚Üí avoids overfitting.\n",
        "* Performance improved: your example shows 87% accuracy, Precision 0.84, Recall 0.82, F1 0.83.\n",
        "\n",
        "Insight:\n",
        "BiLSTM captures sequential context (e.g., ‚Äúnot good‚Äù vs. ‚Äúgood‚Äù), leading to higher recall than TF-IDF + Logistic Regression. This means fewer toxic comments slip through, at the cost of more complexity and training time."
      ],
      "metadata": {
        "id": "lQyCdRnSszL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4Ô∏è‚É£Step 5: Model Comparison**"
      ],
      "metadata": {
        "id": "DULemWU1y7nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assign the accuracy values (replace with the printed ones if needed)\n",
        "lr_acc = accuracy_score(y_val, val_pred_lr)         # Logistic Regression accuracy\n",
        "bilstm_acc = accuracy_score(y_val, val_pred_dl)     # BiLSTM accuracy\n",
        "\n",
        "# Create comparison DataFrame\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Logistic Regression\", \"BiLSTM\"],\n",
        "    \"Accuracy\": [lr_acc, bilstm_acc]\n",
        "})\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(results)\n",
        "\n",
        "# Plot comparison\n",
        "plt.bar(results[\"Model\"], results[\"Accuracy\"], color=['skyblue','orange'])\n",
        "plt.title(\"Model Comparison: Toxic Comment Classification\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SKE4uzqQJAzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight:\n",
        "* BiLSTM consistently outperforms Logistic Regression.\n",
        "* Biggest gain is in Recall (0.82 vs 0.75) ‚Üí the DL model is better at catching toxic comments.\n",
        "* Logistic Regression is still valuable when you need speed + low compute cost (e.g., real-time deployment on edge devices)."
      ],
      "metadata": {
        "id": "L-uAjoUAs7Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace with your actual results from Step 2 & Step 3\n",
        "logreg_acc, logreg_p, logreg_r, logreg_f1 = 0.83, 0.79, 0.75, 0.77\n",
        "bilstm_acc, bilstm_p, bilstm_r, bilstm_f1 = 0.87, 0.84, 0.82, 0.83\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Logistic Regression\", \"BiLSTM\"],\n",
        "    \"Accuracy\": [logreg_acc, bilstm_acc],\n",
        "    \"Precision\": [logreg_p, bilstm_p],\n",
        "    \"Recall\": [logreg_r, bilstm_r],\n",
        "    \"F1-Score\": [logreg_f1, bilstm_f1]\n",
        "})\n",
        "\n",
        "print(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "nQS1JH1aESJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5Ô∏è‚É£Save Best Model for Deployment**"
      ],
      "metadata": {
        "id": "g7Fo9AHazfFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For Logistic Regression (Scikit-learn)**"
      ],
      "metadata": {
        "id": "_vqxnAZRHVh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, joblib\n",
        "\n",
        "# Make sure \"models\" folder exists\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# Save TF-IDF vectorizer + Logistic Regression model\n",
        "joblib.dump(tfidf, \"models/tfidf_vectorizer.joblib\")\n",
        "joblib.dump(logreg, \"models/logreg_toxicity.joblib\")\n",
        "\n",
        "print(\"‚úÖ Logistic Regression model saved successfully\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xDID_bL_DxMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Both models + tokenizers are saved (joblib for TF-IDF + Logistic Regression, .h5 + tokenizer.json for BiLSTM).\n",
        "* This enables easy integration with Streamlit app or deployment as an API.\n",
        "\n",
        "Insight:\n",
        "* Having both models saved lets you choose trade-offs:\n",
        "* Use Logistic Regression for fast, lightweight deployments.\n",
        "* Use BiLSTM for higher accuracy but heavier compute."
      ],
      "metadata": {
        "id": "P-RLyA0ntDXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For BiLSTM (Keras/TensorFlow)**"
      ],
      "metadata": {
        "id": "E22aqpEAHYnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, json\n",
        "\n",
        "# Save Logistic Regression + TF-IDF\n",
        "joblib.dump(tfidf, \"models/tfidf_vectorizer.joblib\")\n",
        "joblib.dump(logreg, \"models/logreg_model.joblib\")\n",
        "\n",
        "# Save BiLSTM model\n",
        "model.save(\"models/bilstm_model.h5\")\n",
        "\n",
        "# Save BiLSTM tokenizer\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open(\"models/bilstm_tokenizer.json\", \"w\") as f:\n",
        "    f.write(tokenizer_json)\n",
        "\n",
        "print(\"‚úÖ All models & tokenizers saved.\")\n"
      ],
      "metadata": {
        "id": "FW1cYwzaLhH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, re, json, joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")    # put your path here if different\n",
        "\n",
        "# Basic checks / clean-up\n",
        "if \"comment_text\" not in df.columns or \"toxic\" not in df.columns:\n",
        "    raise ValueError(\"train.csv must contain columns: 'comment_text' and 'toxic'\")\n",
        "\n",
        "# drop rows with missing critical fields\n",
        "df = df.dropna(subset=[\"comment_text\", \"toxic\"]).copy()\n",
        "\n",
        "# ensure labels are 0/1 integers\n",
        "df[\"toxic\"] = pd.to_numeric(df[\"toxic\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "df[\"toxic\"] = df[\"toxic\"].clip(0, 1)\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    t = str(t).lower()\n",
        "    t = re.sub(r\"http\\S+|www\\.\\S+\", \" \", t)        # URLs\n",
        "    t = re.sub(r\"@\\w+|#\\w+\", \" \", t)               # @mentions, #hashtags\n",
        "    t = re.sub(r\"[^a-z0-9\\s']\", \" \", t)            # keep letters/digits/apostrophes\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "df[\"clean_comment\"] = df[\"comment_text\"].astype(str).apply(clean_text)\n",
        "\n",
        "X_text = df[\"clean_comment\"].values\n",
        "y      = df[\"toxic\"].values\n",
        "\n",
        "# if labels are extremely imbalanced, stratify helps\n",
        "X_train_text, X_val_text, y_train, y_val = train_test_split(\n",
        "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=100_000,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=2,\n",
        "    sublinear_tf=True,\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "X_val_tfidf   = tfidf.transform(X_val_text)\n",
        "\n",
        "print(\"TF-IDF shapes:\", X_train_tfidf.shape, X_val_tfidf.shape)\n",
        "\n",
        "\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "cw_dict = {int(c): w for c, w in zip(classes, class_weights)}\n",
        "print(\"LR class weights:\", cw_dict)\n",
        "\n",
        "logreg = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    class_weight=cw_dict,\n",
        "    n_jobs=-1,\n",
        "    C=4.0,\n",
        "    solver='lbfgs'\n",
        ")\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate LR\n",
        "val_pred_lr  = logreg.predict(X_val_tfidf)\n",
        "val_proba_lr = logreg.predict_proba(X_val_tfidf)[:, 1]\n",
        "\n",
        "lr_acc = accuracy_score(y_val, val_pred_lr)\n",
        "lr_p, lr_r, lr_f1, _ = precision_recall_fscore_support(y_val, val_pred_lr, average='binary', zero_division=0)\n",
        "print(f\"\\n[LogReg] Acc: {lr_acc:.4f}  Precision: {lr_p:.4f}  Recall: {lr_r:.4f}  F1: {lr_f1:.4f}\")\n",
        "print(\"\\n[LogReg] Classification Report:\\n\", classification_report(y_val, val_pred_lr, digits=4))\n",
        "\n",
        "cm = confusion_matrix(y_val, val_pred_lr)\n",
        "plt.imshow(cm, cmap=\"Blues\"); plt.title(\"Confusion Matrix ‚Äî Logistic Regression\")\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
        "for (i,j),z in np.ndenumerate(cm): plt.text(j, i, str(z), ha='center', va='center')\n",
        "plt.colorbar(); plt.show()\n",
        "\n",
        "MAX_WORDS = 60_000\n",
        "MAX_LEN   = 120\n",
        "\n",
        "tokenizer_lstm = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer_lstm.fit_on_texts(X_train_text)\n",
        "\n",
        "X_train_seq = tokenizer_lstm.texts_to_sequences\n"
      ],
      "metadata": {
        "id": "S1EmJHxGzjK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6Ô∏è‚É£Inference Functions**"
      ],
      "metadata": {
        "id": "mb8fE_ztzlBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression Inference\n",
        "def predict_toxic_lr(texts):\n",
        "    texts_clean = [clean_text(t) for t in texts]\n",
        "    X = tfidf.transform(texts_clean)\n",
        "    probs = logreg.predict_proba(X)[:,1]\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    return preds, probs\n",
        "\n",
        "# BiLSTM Inference\n",
        "def predict_toxic_bilstm(texts):\n",
        "    texts_clean = [clean_text(t) for t in texts]\n",
        "    seqs = tokenizer.texts_to_sequences(texts_clean)\n",
        "    pads = pad_sequences(seqs, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "    probs = model.predict(pads).ravel()\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    return preds, probs\n",
        "\n",
        "# Quick test\n",
        "samples = [\"I love this!\", \"You are the worst, shut up.\"]\n",
        "print(\"LR:\", predict_toxic_lr(samples))\n",
        "print(\"BiLSTM:\", predict_toxic_bilstm(samples))\n"
      ],
      "metadata": {
        "id": "Bh0_xFMaLttM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, joblib\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# Save TF-IDF + Logistic Regression\n",
        "joblib.dump(tfidf, \"models/tfidf_vectorizer.joblib\")\n",
        "joblib.dump(logreg, \"models/logreg_toxicity.joblib\")\n",
        "\n",
        "# Save BiLSTM model\n",
        "model.save(\"models/bilstm_model.h5\")\n",
        "\n",
        "# Save BiLSTM tokenizer\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open(\"models/bilstm_tokenizer.json\", \"w\") as f:\n",
        "    f.write(tokenizer_json)\n",
        "\n",
        "print(\"‚úÖ All models saved inside 'models/' folder\")\n"
      ],
      "metadata": {
        "id": "aZJdMMjfQW-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* predict_toxic_lr(texts) ‚Üí clean ‚Üí TF-IDF ‚Üí Logistic Regression.\n",
        "* predict_toxic_bilstm(texts) ‚Üí clean ‚Üí sequence pad ‚Üí BiLSTM.\n",
        "\n",
        "Sample test:\n",
        "* \"I love this!\" ‚Üí predicted non-toxic.\n",
        "* \"You are the worst, shut up.\" ‚Üí predicted toxic.\n",
        "\n",
        "Insight:\n",
        "Inference pipeline is modular ‚Äî you can quickly plug either model into your Streamlit front-end. For a demo, you can even offer both models and let users compare predictions live."
      ],
      "metadata": {
        "id": "HBXdiYp_tUJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"models/tfidf_vectorizer.joblib\")\n",
        "files.download(\"models/logreg_toxicity.joblib\")\n",
        "files.download(\"models/bilstm_model.h5\")\n",
        "files.download(\"models/bilstm_tokenizer.json\")\n"
      ],
      "metadata": {
        "id": "Ua3-Jev8RHUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ***6. Conclusion***\n"
      ],
      "metadata": {
        "id": "-FyyNfD_pnEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully demonstrates how data-driven analysis can improve the detection and moderation of toxic online comments. By integrating text preprocessing, machine learning models, deep learning architectures, and an interactive Streamlit dashboard, we were able to:\n",
        "* Build and compare models such as Logistic Regression and BiLSTM for toxicity detection.\n",
        "* Process and clean large volumes of text to handle noise, stopwords, and inconsistent formatting.\n",
        "* Visualize toxic vs non-toxic comment distributions, common toxic words, and temporal trends.\n",
        "* Provide an interactive platform that allows single comment predictions as well as bulk CSV uploads for large-scale moderation.\n",
        "* Track model performance through metrics like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "Through this analysis, we gain valuable insights that can help social media platforms, online communities, and content moderators:\n",
        "* Reduce harmful interactions by flagging toxic comments in real time.\n",
        "* Optimize moderation workflows by focusing on high-risk comments.\n",
        "* Promote healthier online environments by encouraging positive discussions."
      ],
      "metadata": {
        "id": "WIa3w3bDOGub"
      }
    }
  ]
}